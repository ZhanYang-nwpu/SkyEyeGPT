# <img src="images/SkyEyeGPT.png" height="60"> SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model

##### Author: Yang Zhan, Zhitong Xiong, Yuan Yuan
This is the official repository for paper **"SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model"**. [paper]

**School of Artificial Intelligence, OPtics, and ElectroNics (iOPEN), Northwestern Polytechnical University**
## Please share a <font color='orange'>STAR â­</font> if this project does help

### You can focus on remote sensing multimodal large language model (Vision-Language) [here](https://github.com/ZhanYang-nwpu/Awesome-Remote-Sensing-Multimodal-Large-Language-Model)

## ğŸ“¢ Latest Updates
- **Jan-18-2024**: paper is released. ğŸ”¥ğŸ”¥
- ğŸ“¦ Chatbot, codebase, datasets, and models coming soon! ğŸš€
---





## ğŸ™ Acknowledgement
Our code is based on [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [shikra](https://github.com/shikras/shikra), and [MiniGPT-v2](https://github.com/Vision-CAIR/MiniGPT-4). We sincerely appreciate their contributions and authors for releasing source codes. We are thankful to EVA and LLaMA2 for releasing their models as open-source contributions. I would like to thank Xiong zhitong and Yuan yuan for helping the manuscript. I also thank the School of Artificial Intelligence, OPtics, and ElectroNics (iOPEN), Northwestern Polytechnical University for supporting this work.




## ğŸ¤– Contact
If you have any questions about this project, please feel free to contact zhanyangnwpu@gmail.com.
